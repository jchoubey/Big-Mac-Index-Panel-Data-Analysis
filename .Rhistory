print(paste0("MSE for the Piecewise Polynomial is: ", mse.4c))
# fitting cubic spline
fit.4d <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = wage.train)
summary(fit.4d)
#Plotting the Regression Line to the scatter plot
ggplot(wage.train, aes(x = age, y = wage)) +
geom_point(alpha = 0.3) +
geom_smooth(method="lm",
formula =  y ~ splines::bs(x, knots = c(25, 40, 60), degree = 3)) +
geom_vline(xintercept = c(25, 40, 60), col = "green") +
labs(title = "Wage Dataset - Cubic Splines",
subtitle = "Predicting 'wage' with a cubic spline")
# calculating the testing error
predict.4d <- predict(fit.4d, newdata = wage.test)
mse.4d <- mean((predict.4d - wage.test$wage)^2)
print(paste0("MSE for the Cubic Spline is: ", mse.4d))
# using cross-validation to get
# fitting smoothing splines
fit.4e <- with(wage.train, smooth.spline(age, wage, df=16))
summary(fit.4e)
plot(wage.train$age, wage.train$wage, cex=.5, col='darkgrey')
title("Wage Dataset - Smoothning Spline")
lines(fit.4e, lwd = 2, col="blue")
# calculating the testing error
predict.4e <- predict(fit.4e, newdata = wage.test, deriv = 1)
mse.4e <- mean((predict.4e$y - wage.test$wage)^2)
print(paste0("MSE for the Smoothning Spline is: ", mse.4e))
mses <- c(mse.4a, mse.4b, mse.4c, mse.4d, mse.4e)
model <- c("Polynomial regression", "Step Regression",
"Piecewise Regression", "Cubic Spline", "Smoothing Splines")
mse.df <- cbind(model, mses)
mse.df %>%
kable("html") %>%
kable_styling(font_size = 12, full_width = FALSE)
# loading the dataset and removing the name column
auto_df <- data.frame(Auto)
auto_df <- subset(auto_df, select = -c(name))
# Make dependent variable as a factor (categorical)
auto_df$origin = as.factor(auto_df$origin)
head(auto_df)
set.seed(1234)
# creating train-test split
auto.train.split <- sample(1:nrow(auto_df), nrow(auto_df) * 0.7)
auto.test <- auto_df[-auto.train.split,]
auto.train <- auto_df[auto.train.split,]
# make our LARGE tree
tree.auto <- tree(mpg ~., auto_df, subset = auto.train.split ,
control = tree.control(nobs = length(auto.train.split), mindev = 0))
summary(tree.auto)
# plotting the tree
plot(tree.auto)
text(tree.auto, pretty = 0)
# calculating the testing error
predict.5a_1 <- predict(tree.auto, newdata = auto.test)
mse.5a_1 <- mean((predict.5a_1 - auto.test$mpg)^2)
print(paste0("MSE for the unpruned tree is: ", mse.5a_1))
# Use Cross-Validation to see if pruning will improve performance
cv.auto <- cv.tree(tree.auto)
plot(cv.auto$size, cv.auto$dev, type = "b")
# Pruning Time by Hand
prune.auto <- prune.tree(tree.auto, best = 7)
plot(prune.auto)
text(prune.auto, pretty = 0)
# calculating the testing error
predict.5a_2 <- predict(prune.auto, newdata = auto.test)
mse.5a_2 <- mean((predict.5a_2 - auto.test$mpg)^2)
print(paste0("MSE for the pruned tree is: ", mse.5a_2))
# bagged regression tree
bag.auto <- randomForest(mpg ~ ., data = auto.train,
mtry = ncol(auto.train) - 1,importance = TRUE)
print(bag.auto)
# calculating the testing error
predict.5b_1 <- predict(bag.auto, newdata = auto.test)
mse.5b_1 <- mean((predict.5b_1 - auto.test$mpg)^2)
print(paste0("MSE for the bagged regression is: ", mse.5b_1))
# predictors importance
importance(bag.auto)
# tuned bagged regression tree
tune.bag.auto <- randomForest(mpg ~ ., data = auto.train, mtry = 5, importance = TRUE)
print(tune.bag.auto)
# calculating the testing error
predict.5b <- predict(tune.bag.auto, newdata = auto.test)
mse.5b <- mean((predict.5b - auto.test$mpg)^2)
print(paste0("MSE for the bagged regression after tuning is: ", mse.5b))
set.seed(1234)
# fitting random forest
rf.auto <- randomForest(mpg ~ ., data = auto.train, importance = TRUE)
rf.auto
# checking the importance of predictors
importance(rf.auto)
# tuned random forest
tune.rf.auto <- randomForest(mpg ~ ., data = auto.train,
mtry = 5, ntree = 700, importance = TRUE)
tune.rf.auto
# again checking the predictors importance
varImpPlot(rf.auto)
# calculating the testing error
predict.5c_1 <- predict(rf.auto, newdata = auto.test)
mse.5c_1 <- mean((predict.5c_1 - auto.test$mpg)^2)
print(paste0("MSE for the Random forest before tuning is: ", mse.5c_1))
# calculating the testing error
predict.5c_2 <- predict(tune.rf.auto, newdata = auto.test)
mse.5c_2 <- mean((predict.5c_2 - auto.test$mpg)^2)
print(paste0("MSE for the Random forest after tuning is: ", mse.5c_2))
# fitting a GAM model
gam.auto <- gam(mpg~s(acceleration)+s(displacement)+s(horsepower)
+s(weight)+s(year), data=auto.train)
summary(gam.auto)
# plotting a gam
plot(gam.auto, se = TRUE, col="blue")
# calculating the testing error
predict.5d <- predict(gam.auto, newdata = auto.test)
mse.5d <- mean((predict.5d - auto.test$mpg)^2)
print(paste0("MSE for the GAMS is: ", mse.5d))
# fitting cubic spline
fit.4d <- lm(wage ~ bs(age, knots = c(25, 40, 60)), data = wage.train)
summary(fit.4d)
#Plotting the Regression Line to the scatter plot
ggplot(wage.train, aes(x = age, y = wage)) +
geom_point(alpha = 0.3) +
geom_smooth(method="lm",
formula =  y ~ splines::bs(x, knots = c(25, 40, 60), degree = 3)) +
geom_vline(xintercept = c(25, 40, 60), col = "green") +
labs(title = "Wage Dataset - Cubic Splines",
subtitle = "Predicting 'wage' with a cubic spline")
# calculating the testing error
predict.4d <- predict(fit.4d, newdata = wage.test)
mse.4d <- mean((predict.4d - wage.test$wage)^2)
print(paste0("MSE for the Cubic Spline is: ", mse.4d))
# using cross-validation to get
# fitting smoothing splines
fit.4e <- with(wage.train, smooth.spline(age, wage, df=16))
summary(fit.4e)
plot(wage.train$age, wage.train$wage, cex=.5, col='darkgrey')
title("Wage Dataset - Smoothning Spline")
lines(fit.4e, lwd = 2, col="blue")
# calculating the testing error
predict.4e <- predict(fit.4e, newdata = wage.test, deriv = 1)
mse.4e <- mean((predict.4e$y - wage.test$wage)^2)
print(paste0("MSE for the Smoothning Spline is: ", mse.4e))
mses <- c(mse.4a, mse.4b, mse.4c, mse.4d, mse.4e)
model <- c("Polynomial regression", "Step Regression",
"Piecewise Regression", "Cubic Spline", "Smoothing Splines")
mse.df <- cbind(model, mses)
mse.df %>%
kable("html") %>%
kable_styling(font_size = 12, full_width = FALSE)
# loading the dataset and removing the name column
auto_df <- data.frame(Auto)
auto_df <- subset(auto_df, select = -c(name))
# Make dependent variable as a factor (categorical)
auto_df$origin = as.factor(auto_df$origin)
head(auto_df)
set.seed(1234)
# creating train-test split
auto.train.split <- sample(1:nrow(auto_df), nrow(auto_df) * 0.7)
auto.test <- auto_df[-auto.train.split,]
auto.train <- auto_df[auto.train.split,]
# make our LARGE tree
tree.auto <- tree(mpg ~., auto_df, subset = auto.train.split ,
control = tree.control(nobs = length(auto.train.split), mindev = 0))
summary(tree.auto)
# plotting the tree
plot(tree.auto)
text(tree.auto, pretty = 0)
# calculating the testing error
predict.5a_1 <- predict(tree.auto, newdata = auto.test)
mse.5a_1 <- mean((predict.5a_1 - auto.test$mpg)^2)
print(paste0("MSE for the unpruned tree is: ", mse.5a_1))
# Use Cross-Validation to see if pruning will improve performance
cv.auto <- cv.tree(tree.auto)
plot(cv.auto$size, cv.auto$dev, type = "b")
# Pruning Time by Hand
prune.auto <- prune.tree(tree.auto, best = 7)
plot(prune.auto)
text(prune.auto, pretty = 0)
# calculating the testing error
predict.5a_2 <- predict(prune.auto, newdata = auto.test)
mse.5a_2 <- mean((predict.5a_2 - auto.test$mpg)^2)
print(paste0("MSE for the pruned tree is: ", mse.5a_2))
# bagged regression tree
bag.auto <- randomForest(mpg ~ ., data = auto.train,
mtry = ncol(auto.train) - 1,importance = TRUE)
print(bag.auto)
# calculating the testing error
predict.5b_1 <- predict(bag.auto, newdata = auto.test)
mse.5b_1 <- mean((predict.5b_1 - auto.test$mpg)^2)
print(paste0("MSE for the bagged regression is: ", mse.5b_1))
# predictors importance
importance(bag.auto)
# tuned bagged regression tree
tune.bag.auto <- randomForest(mpg ~ ., data = auto.train, mtry = 5, importance = TRUE)
print(tune.bag.auto)
# calculating the testing error
predict.5b <- predict(tune.bag.auto, newdata = auto.test)
mse.5b <- mean((predict.5b - auto.test$mpg)^2)
print(paste0("MSE for the bagged regression after tuning is: ", mse.5b))
set.seed(1234)
# fitting random forest
rf.auto <- randomForest(mpg ~ ., data = auto.train, importance = TRUE)
rf.auto
# checking the importance of predictors
importance(rf.auto)
# tuned random forest
tune.rf.auto <- randomForest(mpg ~ ., data = auto.train,
mtry = 5, ntree = 700, importance = TRUE)
tune.rf.auto
# again checking the predictors importance
varImpPlot(rf.auto)
# calculating the testing error
predict.5c_1 <- predict(rf.auto, newdata = auto.test)
mse.5c_1 <- mean((predict.5c_1 - auto.test$mpg)^2)
print(paste0("MSE for the Random forest before tuning is: ", mse.5c_1))
# calculating the testing error
predict.5c_2 <- predict(tune.rf.auto, newdata = auto.test)
mse.5c_2 <- mean((predict.5c_2 - auto.test$mpg)^2)
print(paste0("MSE for the Random forest after tuning is: ", mse.5c_2))
# fitting a GAM model
gam.auto <- gam(mpg~s(acceleration)+s(displacement)+s(horsepower)
+s(weight)+s(year), data=auto.train)
summary(gam.auto)
# plotting a gam
plot(gam.auto, se = TRUE, col="blue")
# calculating the testing error
predict.5d <- predict(gam.auto, newdata = auto.test)
mse.5d <- mean((predict.5d - auto.test$mpg)^2)
print(paste0("MSE for the GAMS is: ", mse.5d))
setwd("D:/Big-Mac-Index-Panel-Data-Analysis")
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(dplyr)     # For dataframe manipulation
library(simcf)     # For panel functions and simulators
library(tseries)   # For ADF unit root test
library(plm)       # Econometrics package for linear panel models
library(nlme)      # Package for estimation - RE models
library(plm)       # Package for estimation - FE models
library(kableExtra)# Beautifying the table
library(ggplot2)   # package for plotting
library(MASS)      # for random nv
df <- read.csv('cleanedData/processed_data_I.csv')
head(df)
Plotting the time series,
# Plotting the time series
ggplot(data = df, aes(x=year, y=big_mac_index)) +
geom_line(aes(colour=iso_a3))
df <- read.csv('cleanedData/processed_data_I.csv')
attach(df)
head(df)
ts <- with(df, data.frame(split(big_mac_index, as.character(year))))
purtest(ts, pmax = 4, exo = "intercept", test = "ips")
ts <- with(df, data.frame(split(big_mac_index, as.character(year))))
purtest(ts, pmax = 4, exo = "trend", test = "ips")
country.bigmac <- aggregate(df$big_mac_index, list(df$iso_a3), FUN=mean)
country.bigmac
library(rworldmap)
install.packages("rworldmap")
library(rworldmap)
#create a map-shaped window
mapDevice('x11')
#join to a coarse resolution map
spdf <- joinCountryData2Map(country.bigmac, joinCode="NAME", nameJoinColumn="Group.1")
mapCountryData(spdf, nameColumnToPlot="value", catMethod="fixedWidth")
barplot(country.bigmac, main="Car Distribution",
xlab="Number of Gears")
barplot(table(country.bigmac), main="Car Distribution",
xlab="Number of Gears")
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity")
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + title("Dollar Valuation by Country")
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + main("Dollar Valuation by Country")
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + main("Dollar Valuation by Country")
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Dollar Valuation by Country")
# group Big mac index by country
year.bigmac <- aggregate(df$big_mac_index, list(df$year), FUN=mean)
# Barplot
ggplot(year.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Dollar Valuation by Country")
# group Big mac index by country
year.bigmac <- aggregate(df$big_mac_index, list(df$year), FUN=mean)
# Barplot
ggplot(year.bigmac, aes(x=Group.1, y=x)) +
geom_line(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Dollar Valuation by Country")
# group Big mac index by country
country.bigmac <- aggregate(df$big_mac_index, list(df$iso_a3), FUN=mean)
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Dollar Valuation by Country")
View(df)
# basic scatterplot
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
# basic scatterplot
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
# basic scatterplot
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
# basic scatterplot
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
# basic scatterplot
ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(dplyr)     # For dataframe manipulation
library(simcf)     # For panel functions and simulators
library(tseries)   # For ADF unit root test
library(plm)       # Econometrics package for linear panel models
library(nlme)      # Package for estimation - RE models
library(plm)       # Package for estimation - FE models
library(kableExtra)# Beautifying the table
library(ggplot2)   # package for plotting
library()
library(MASS)      # for random nv
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(dgp, ca, labcomp, ggb, ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
install.packages("gridExtra")
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(dplyr)     # For dataframe manipulation
library(simcf)     # For panel functions and simulators
library(tseries)   # For ADF unit root test
library(plm)       # Econometrics package for linear panel models
library(nlme)      # Package for estimation - RE models
library(plm)       # Package for estimation - FE models
library(kableExtra)# Beautifying the table
library(ggplot2)   # package for plotting
library(gridExtra)
library(MASS)      # for random nv
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(dgp, ca, labcomp, ggb, ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(gdp, ca, labcomp, ggb, ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(dplyr)     # For dataframe manipulation
library(simcf)     # For panel functions and simulators
library(tseries)   # For ADF unit root test
library(plm)       # Econometrics package for linear panel models
library(nlme)      # Package for estimation - RE models
library(plm)       # Package for estimation - FE models
library(kableExtra)# Beautifying the table
library(ggplot2)   # package for plotting
library(gridExtra)
library(MASS)      # for random nv
df <- read.csv('cleanedData/processed_data_I.csv')
attach(df)
head(df)
# Plotting the time series
ggplot(data = df, aes(x=year, y=big_mac_index)) +
geom_line(aes(colour=iso_a3))
ts <- with(df, data.frame(split(big_mac_index, as.character(year))))
purtest(ts, pmax = 4, exo = "intercept", test = "ips")
ts <- with(df, data.frame(split(big_mac_index, as.character(year))))
purtest(ts, pmax = 4, exo = "trend", test = "ips")
# group Big mac index by country
country.bigmac <- aggregate(df$big_mac_index, list(df$iso_a3), FUN=mean)
# Barplot
ggplot(country.bigmac, aes(x=Group.1, y=x)) +
geom_bar(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Dollar Valuation by Country")
# group Big mac index by country
year.bigmac <- aggregate(df$big_mac_index, list(df$year), FUN=mean)
# Barplot
ggplot(year.bigmac, aes(x=Group.1, y=x)) +
geom_line(stat = "identity") +
xlab("Country") + ylab("Big Mac Index") + ggtitle("Annual Dollar Valuation")
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(gdp, ca, labcomp, ggb, ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
knitr::opts_chunk$set(include = TRUE, warning = FALSE, message = FALSE)
library(dplyr)     # For dataframe manipulation
library(simcf)     # For panel functions and simulators
library(tseries)   # For ADF unit root test
library(plm)       # Econometrics package for linear panel models
library(nlme)      # Package for estimation - RE models
library(plm)       # Package for estimation - FE models
library(kableExtra)# Beautifying the table
library(ggplot2)   # package for plotting
library(gridExtra)
library(grid)
library(MASS)      # for random nv
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(gdp, ca, labcomp, ggb, ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
# basic scatterplot
gdp <- ggplot(df, aes(x=GDP.per.capita, y=big_mac_index)) +
geom_point()
ca <- ggplot(df, aes(x=Curr.Acc.Balance, y=big_mac_index)) +
geom_point()
labcomp <- ggplot(df, aes(x=labor.comp.per.capita, y=big_mac_index)) +
geom_point()
ggb <- ggplot(df, aes(x=GGB, y=big_mac_index)) +
geom_point()
grid.arrange(grobs = list(gdp, ca, labcomp, ggb), ncol=2,
main="Absolute valuation vs Big Mac determinants graph")
countrylist <- unique(countrylist)
countrylist <- unique(Country.Code)
# Look at the ACF for each country
for (i in 1:length(countrylist)) {
currcty <- countrylist[i]
acf(df$big_mac_index[COUNTRY==currcty])
dev.off()
}
# Look at the ACF for each country
for (i in 1:length(countrylist)) {
currcty <- countrylist[i]
acf(df$big_mac_index[Country.Code==currcty])
dev.off()
}
# Look at the PACF
for (i in 1:length(countrylist)) {
currcty <- countrylist[i]
pacf(df$big_mac_index[Country.Code==currcty])
dev.off()
}
model <- big_mac_index ~ GDP.per.capita + Curr.Acc.Balance + GGB + labor.comp.per.capita
wi <- plm(model, data = df, model = "within")
model <- big_mac_index ~ GDP.per.capita + Curr.Acc.Balance + GGB + labor.comp.per.capita
wi <- plm(big_mac_index ~ GDP.per.capita + Curr.Acc.Balance + GGB + labor.comp.per.capita, data = df, model = "within")
model <- big_mac_index ~ GDP.per.capita + Curr.Acc.Balance + GGB + labor.comp.per.capita
wi <- plm(model, data = df, model = "within")
model <- big_mac_index ~ GDP.per.capita + Curr.Acc.Balance + GGB + labor.comp.per.capita
pooled_ols_plm <- plm(model, data = df,
index = c("Country.Code", "year"),
effect = "individual", model = "pooling")
summary(pooled_ols_plm)
fe_model_lm <- lm(model, data = df)
summary(fe_model_lm)
fe_model_plm <- plm(model, data = df,
index = c("Country.Code", "year"),
effect = "individual", model = "within")
summary(fe_model_plm)
re_model_plm <- plm(model, data = df,
index = c("Country.Code", "year"),
effect = "individual", model = "random")
summary(re_model_plm)
phtest(fe_model_plm, re_model_plm)
